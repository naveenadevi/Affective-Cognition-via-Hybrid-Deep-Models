!pip install tensorflow
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import nltk
import string
import json
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
# Load datasets
def load_data(file_path):
    df = pd.read_csv(file_path, header=None, names=["data"])
    df[["tweet", "label"]] = df["data"].str.split(";", expand=True)
    df.drop(columns=["data"], inplace=True)
    return df
from google.colab import drive
drive.mount('/content/drive')
import os

print(os.path.exists("/content/drive/MyDrive/train.txt"))  # Check train.txt
print(os.path.exists("/content/drive/MyDrive/val.txt"))    # Check val.txt
print(os.path.exists("/content/drive/MyDrive/test.txt"))   # Check test.txt
train_df = load_data("/content/drive/MyDrive/train.txt")
val_df = load_data("/content/drive/MyDrive/val.txt")
test_df = load_data("/content/drive/MyDrive/test.txt")
# Handle missing values
train_df.dropna(inplace=True)
val_df.dropna(inplace=True)
test_df.dropna(inplace=True)
# Text preprocessing
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = text.translate(str.maketrans("", "", string.punctuation))  # Remove punctuation
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize
    return " ".join(words)
train_df["tweet"] = train_df["tweet"].apply(preprocess_text)
val_df["tweet"] = val_df["tweet"].apply(preprocess_text)
test_df["tweet"] = test_df["tweet"].apply(preprocess_text)
# Encode labels
label_encoder = LabelEncoder()
train_df["label"] = label_encoder.fit_transform(train_df["label"])
val_df["label"] = label_encoder.transform(val_df["label"])
test_df["label"] = label_encoder.transform(test_df["label"])
# Save label encoder classes
with open("label_encoder.json", "w") as f:
    json.dump(label_encoder.classes_.tolist(), f)
# Tokenization
max_words = 10000  # Max vocabulary size
max_len = 128  # Max sequence length
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(train_df["tweet"])
# Convert text to sequences
train_sequences = tokenizer.texts_to_sequences(train_df["tweet"])
val_sequences = tokenizer.texts_to_sequences(val_df["tweet"])
test_sequences = tokenizer.texts_to_sequences(test_df["tweet"])
# Padding sequences
train_padded = pad_sequences(train_sequences, maxlen=max_len, padding="post", truncating="post")
val_padded = pad_sequences(val_sequences, maxlen=max_len, padding="post", truncating="post")
test_padded = pad_sequences(test_sequences, maxlen=max_len, padding="post", truncating="post")
# Convert labels to categorical (one-hot encoding)
num_classes = len(label_encoder.classes_)
train_labels = to_categorical(train_df["label"], num_classes=num_classes)
val_labels = to_categorical(val_df["label"], num_classes=num_classes)
test_labels = to_categorical(test_df["label"], num_classes=num_classes)
# Build BiGRU model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, dropout=0.3, recurrent_dropout=0.3)),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dropout(0.3),  # Prevents overfitting
    tf.keras.layers.Dense(num_classes, activation="softmax")
])
# Compile model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
# Implement early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
# Train model
epochs = 5
batch_size = 64
history = model.fit(
    train_padded, train_labels,
    validation_data=(val_padded, val_labels),
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stopping]
)
# Evaluate on test set
test_loss, test_acc = model.evaluate(test_padded, test_labels)
print(f"Test Accuracy: {test_acc:.4f}")
y_pred = model.predict(test_padded)  # Get model predictions
y_pred = y_pred.flatten()  # Flatten if needed

y_true = test_labels.flatten()  # Ensure the labels match shape
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_true, y_pred)
print(f'MSE: {mse}')
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_true, y_pred)
print(f'MAE: {mae}')
import numpy as np

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    nonzero_indices = y_true != 0  # Mask to filter out zero values
    return np.mean(np.abs((y_true[nonzero_indices] - y_pred[nonzero_indices]) / y_true[nonzero_indices])) * 100

mape = mean_absolute_percentage_error(y_true, y_pred)
print(f'MAPE: {mape}%')
# Save model and tokenizer
model.save("bigru_emotion_model.keras")  # Recommended Keras format
tokenizer_json = tokenizer.to_json()
with open("tokenizer.json", "w") as f:
    f.write(tokenizer_json)

print("Model and tokenizer saved successfully!")
# Load the trained model and tokenizer for real-time predictions
model = tf.keras.models.load_model("bigru_emotion_model.keras")
# Load tokenizer correctly
from tensorflow.keras.preprocessing.text import tokenizer_from_json

with open("tokenizer.json", "r") as f:
    tokenizer_json = json.load(f)

# Convert dictionary to JSON string before loading
tokenizer = tokenizer_from_json(json.dumps(tokenizer_json))

with open("label_encoder.json", "r") as f:
    label_classes = json.load(f)

label_encoder = LabelEncoder()
label_encoder.classes_ = np.array(label_classes)

# Function to predict emotion from user input
def predict_emotion(text):
    processed_text = preprocess_text(text)  # Preprocess input text
    sequence = tokenizer.texts_to_sequences([processed_text])  # Convert to sequence
    padded_sequence = pad_sequences(sequence, maxlen=128, padding="post", truncating="post")  # Pad sequence

    prediction = model.predict(padded_sequence)  # Get model prediction
    predicted_label = np.argmax(prediction)  # Get label index
    emotion = label_encoder.inverse_transform([predicted_label])[0]  # Convert index to label

    return emotion

# Take user input and predict emotion
while True:
    user_input = input("Enter a sentence (or type 'exit' to quit): ")
    if user_input.lower() == "exit":
        print("Exiting...")
        break
    emotion = predict_emotion(user_input)
    print(f"Predicted Emotion: {emotion}")
